# Three Generations Seeking Romance â€” Age & Generation Prediction using Regression & Classification  
Using Machine Learning & NLP on the OKCupid Dataset  

---

## ğŸ“Œ Project Overview  
This project explores whether a user's online dating profile can predict:

1. **Their age** (Regression)  
2. **Their generation** â€” *Millennial, Gen X, or Boomer* (Classification)

Using a dataset of ~60,000 anonymized OKCupid user profiles, we apply data cleaning, feature engineering, NLP (TFâ€“IDF), and machine learning models to build predictive systems for both tasks.

At the time the dataset was created (~2011â€“2012), generations were defined as:
- **Millennial:** 18â€“32  
- **Gen X:** 33â€“47  
- **Boomer:** 48â€“70  

---

## ğŸ“‚ Dataset  
The OKCupid dataset contains both structured and unstructured features:

### **Structured (categorical/numeric):**
- age, sex, body_type, diet, drinks, drugs  
- education, job, ethnicity, religion  
- height, income, orientation, status  
- location, sign, smokes  

### **Unstructured text:**
- **essay0 â€“ essay9** (long-form personal descriptions)

The text features are later merged into a single field: `essay_all`.

---

## ğŸ§¹ 1. Data Preparation  

### **1.1 Missing Values**
- Filled numerical missing values:
  - `height` â†’ median  
  - `income` â†’ -1 (unknown)
- Filled categorical missing values with `"Unknown"`
- Filled essay fields with empty strings  
- Dropped users with missing age values  

### **1.2 Essay Consolidation**
- Combined all `essay0`â€“`essay9` columns into a unified text column: `essay_all`
- Removed the original essay columns

### **1.3 Generation Creation**
Created a new classification target using age ranges:

| Age Range | Generation |
|-----------|------------|
| 18â€“32     | Millennial |
| 33â€“47     | Gen X      |
| 48â€“70     | Boomer     |

Encoded as `generation_encoded` for modeling.

---

## ğŸ› ï¸ 2. Feature Engineering & Encoding  

### **TFâ€“IDF for Text**
The combined essay text (`essay_all`) was converted to a numerical representation via:  
```python TfidfVectorizer(max_features=500, stop_words="english") ```python
### **One-Hot Encoding for Categorical Variables**
Categorical features were encoded using:
```python OneHotEncoder(handle_unknown="ignore")```python

Numeric Features (Passthrough)
Numeric variables such as height, income, etc. were passed directly to the model without any scaling, as tree-based models do not require normalization.
ColumnTransformer Workflow
A unified ColumnTransformer was built to process all feature types simultaneously:
Numeric features â†’ passthrough
Categorical features â†’ OneHotEncoder
Text feature (essay_all) â†’ TF-IDF with 500 features
This automated preprocessing ensures consistent transformation during training and prediction.

ğŸ¤– ## 3. Modeling
3.1 Trainâ€“Test Split
Both regression (age prediction) and classification (generation prediction) tasks used an 80/20 split:
train_test_split(X, y, test_size=0.2, random_state=42)
3.2 Regression Models (Age Prediction)
Regression models trained:
Gradient Boosting Regressor
Random Forest Regressor
Linear Regression
Metrics Used
MAE (Mean Absolute Error)
MSE (Mean Squared Error)

â¡ï¸ Gradient Boosting Regressor performed the best.

3.3 Classification Models (Generation Prediction)
Classification models trained:
Random Forest Classifier
Gradient Boosting Classifier
Logistic Regression
Metric Used
Accuracy

â¡ï¸ Logistic Regression achieved the highest accuracy (~69%).

ğŸ“Š ## Visualizations
The notebook also includes:
Age histogram
Generation countplot
Confusion matrices
Performance comparison tables
These visuals help interpret both the dataset and model performance.

ğŸ“ˆ ## Key Insights
Text features (essays) add significant predictive value when processed with TF-IDF.
Age prediction achieves an average error of 5â€“6 years, which is strong considering noisy user-generated text.
Generation classification achieves ~68â€“69% accuracy, indicating moderate predictability.
Ensemble models (Gradient Boosting, Random Forest) perform consistently well.
Logistic Regression surprisingly outperforms tree models for generation classification.

ğŸ› ï¸ ## Technologies Used
Python
Pandas, NumPy
Scikit-learn
TF-IDF vectorization
Matplotlib, Seaborn
